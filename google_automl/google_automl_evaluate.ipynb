{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AutoML Evaluation\n",
    "At the time of execution, Google AutoML was trainable through the UI, and then you can evaluate the test dataset offline as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import automl_v1beta1\n",
    "from google.cloud.automl_v1beta1.proto import service_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params\n",
    "This script assumes you already have an AutoML project set up with data uploaded and run through training. The script will then connect to AutoML and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_sets = []  # List ones to skip here\n",
    "root_data = '{PATH_TO_ROOT_DATASET_DIR}''\n",
    "max_predict = 1000\n",
    "project_id = '{PROJECT_NAME}'\n",
    "skip_tests = []\n",
    "\n",
    "# List of dataset name and model IDs that were trained (from rsanders2@urbn.com account)\n",
    "dataset_models = {\n",
    "     'fashion_mnist_10p': '{DATASET_API_KEY}',    \n",
    "     'cifar10_20p': '{DATASET_API_KEY}',    \n",
    "     'uo_dress': '{DATASET_API_KEY}',\n",
    "     'fashion_mnist_tiny': '{DATASET_API_KEY}',    \n",
    "     'cifar10_tiny': '{DATASET_API_KEY}',    \n",
    "     'uo_dress_tiny': '{DATASET_API_KEY}',  \n",
    "     'mnist_tiny': '{DATASET_API_KEY}',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction API Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(file_path, project_id, model_id):\n",
    "    prediction_client = automl_v1beta1.PredictionServiceClient()\n",
    "    with open(file_path, 'rb') as ff:\n",
    "        content = ff.read()\n",
    "\n",
    "    name = 'projects/{}/locations/us-central1/models/{}'.format(project_id, model_id)\n",
    "    payload = {\n",
    "        'image': {\n",
    "            'image_bytes': content\n",
    "        }\n",
    "    }\n",
    "    params = {'score_threshold': '0'}\n",
    "    request = prediction_client.predict(name, payload, params)\n",
    "    return request  # waits till request is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to evaluate dataset using AutoML model given above params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_path, project_id, model_id):\n",
    "    \n",
    "    d = os.path.basename(dataset_path)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    project_name = '{}_{}'.format(d, timestamp)    \n",
    "    \n",
    "    # parse data\n",
    "    labels = pd.read_csv(os.path.join(dataset_path, 'labels.csv'), header=None, dtype=str)\n",
    "    train_files = labels[labels[0].str.contains('(train|val)')].values\n",
    "    test_files = labels[labels[0].str.contains('test')].values    \n",
    "    \n",
    "    y_train = train_files[:,1]\n",
    "    class_labels = np.sort(np.unique(y_train))\n",
    "    \n",
    "    # Assign ground truth for test\n",
    "    y_true = test_files[:,1]\n",
    "\n",
    "    # Load in files for prediction \n",
    "    y_pred = []\n",
    "    scores = []\n",
    "    files = {}\n",
    "    test_filenames = test_files[:, 0]\n",
    "    test_filenames = [os.path.join(dataset_path, c) for c in test_filenames]\n",
    "    n_pred = len(test_filenames)\n",
    "    print('{} prediction files found.'.format(n_pred))\n",
    "    \n",
    "    # Clean up sizing\n",
    "    if n_pred > max_predict:\n",
    "        print('Number of prediction files ({}) exceeds maximum ({}). sampling by class down to max.'.format(n_pred, max_predict))\n",
    "        inds=np.random.choice(n_pred, max_predict, replace=False)\n",
    "        y_true = y_true[inds]\n",
    "        test_filenames = np.array(test_filenames)[inds].tolist() \n",
    "        print(\"New length: {}\".format(len(test_filenames)))\n",
    "    \n",
    "    # Worker for predictions\n",
    "    def predict_worker(filename):\n",
    "        basename = os.path.basename(filename)      \n",
    "        t_pred = None\n",
    "        t_scores = None         \n",
    "        try:\n",
    "            p = get_prediction(filename, project_id, model_id)\n",
    "            out = MessageToDict(p)\n",
    "            pred_set = out['payload']          \n",
    "            \n",
    "            # Get sorted scores from prediction set           \n",
    "            t_names = np.array([z['displayName'] for z in pred_set])\n",
    "            t_scores = np.array([z['classification']['score'] for z in pred_set])\n",
    "            sort_inds = np.argsort(t_names)\n",
    "            t_names = t_names[sort_inds]\n",
    "            t_scores = t_scores[sort_inds]\n",
    "            t_names = t_names[1::] # remove --other--\n",
    "            t_scores = t_scores[1::] # remove --other--\n",
    "            t_pred = t_names[np.argmax(t_scores)]        \n",
    "        except:\n",
    "            print('Error occured on prediction: {}. Skipping save.'.format(filename))\n",
    "\n",
    "        return basename, t_pred, t_scores\n",
    "\n",
    "    # Run parallel calls to make faster\n",
    "    t_start = time.time()\n",
    "    with Parallel(n_jobs=-1, verbose=5) as parallel:\n",
    "        results = parallel(delayed(predict_worker)(f) for f in test_filenames)\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print(len(results))\n",
    "    print('{:0.3f} secs elapsed for predicting {} images'.format(t_elapsed, len(test_filenames)))\n",
    "\n",
    "    # Parse the parallel output\n",
    "    returned_files = [r[0] for r in results]\n",
    "    y_pred = [r[1] for r in results]\n",
    "    scores = [r[2] for r in results]\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    print('Number of predictions: {}'.format(len(y_pred)))\n",
    "    print('Number of fails: {}.'.format(sum([yy is None for yy in y_pred])))\n",
    "\n",
    "    #\n",
    "    # Save results\n",
    "    #\n",
    "    save_file = '{}-results.p'.format(project_name)\n",
    "    save_dict = {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'scores': scores,\n",
    "        'class_labels': class_labels,\n",
    "        'model_name': project_name,\n",
    "        'model': None,\n",
    "        'train_files': train_files,\n",
    "        'test_files': test_files,\n",
    "        'returned_files': returned_files\n",
    "        }\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    print('Saved to {}'.format(save_file))\n",
    "\n",
    "    return save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through datasets and run AutoML evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,model_id in dataset_models.items():\n",
    "    if d in skip_sets:\n",
    "        continue\n",
    "        \n",
    "    # Get dataset key and directory\n",
    "    dataset_path = os.path.join(root_data, d)\n",
    "    print('EXECUTING DATASET: {}'.format(dataset_path))    \n",
    "    \n",
    "    # Run mega routine\n",
    "    evaluate(dataset_path, project_id, model_id)\n",
    "    \n",
    "    # Output\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
