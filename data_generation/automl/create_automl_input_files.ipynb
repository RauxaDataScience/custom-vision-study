{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AutoML Input Generation\n",
    "Google AutoML allows a CSV upload for input data. This takes the datasets and converts them to google automl input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets of interest\n",
    "datasets = [\n",
    "    'fashion_mnist_tiny',    \n",
    "    'cifar10_tiny', \n",
    "    'uo_dress_tiny',\n",
    "    'mnist_tiny'\n",
    "]\n",
    "root_data = '{PATH_TO_ROOT_DATASETS}'\n",
    "remote_dir = 'gs://PATH/TO/BUCKET/WHERE/DATA/LIVES'\n",
    "max_test = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the input datasets\n",
    "Loops through each dataset and creates input CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    dataset_path = os.path.join(root_data, d)\n",
    "    remote_path = os.path.join(remote_dir, d)\n",
    "\n",
    "    # Read in file locations and labels\n",
    "    labels = pd.read_csv(os.path.join(dataset_path, 'labels.csv'), header=None, dtype=str)\n",
    "    \n",
    "    # Train\n",
    "    train_files = labels[labels[0].str.contains('train/')].values\n",
    "    train_files[:,0] = [os.path.join(remote_path,t) for t in train_files[:,0]]\n",
    "    if len(train_files[0,1])==1:\n",
    "        # turn into an actual entry supported by google\n",
    "        train_files[:,1] = ['a_{}'.format(t) for t in train_files[:,1]]\n",
    "    train_new = []\n",
    "    for i,t in enumerate(train_files):\n",
    "        train_new += [['TRAIN', t[0], t[1]]]\n",
    "    train_files = train_new\n",
    "    \n",
    "    # Lazy copy paste for validation\n",
    "    val_files = labels[labels[0].str.contains('val/')].values\n",
    "    val_files[:,0] = [os.path.join(remote_path,t) for t in val_files[:,0]]\n",
    "    if len(val_files[0,1])==1:\n",
    "        # turn into an actual entry supported by google\n",
    "        val_files[:,1] = ['a_{}'.format(t) for t in val_files[:,1]]\n",
    "    val_new = []\n",
    "    for i,t in enumerate(val_files):\n",
    "        val_new += [['VALIDATION', t[0], t[1]]]\n",
    "    val_files = val_new    \n",
    "    \n",
    "    # Lazy copy paste for test\n",
    "    test_files = labels[labels[0].str.contains('test/')].values\n",
    "    test_files[:,0] = [os.path.join(remote_path,t) for t in test_files[:,0]]\n",
    "    keep_inds = np.random.choice(len(test_files), max_test, replace=False)\n",
    "    test_files = test_files[keep_inds, :]\n",
    "    if len(test_files[0,1])==1:\n",
    "        # turn into an actual entry supported by google\n",
    "        test_files[:,1] = ['a_{}'.format(t) for t in test_files[:,1]]\n",
    "    test_new = []\n",
    "    for i,t in enumerate(test_files):\n",
    "        test_new += [['TEST', t[0], t[1]]]\n",
    "    test_files = test_new        \n",
    "    \n",
    "    files = train_files + val_files + test_files \n",
    "    \n",
    "    # Write file\n",
    "    labels_file = os.path.join(dataset_path, 'google_inputs.csv')    \n",
    "    with open(labels_file, \"w\") as fid:\n",
    "        writer = csv.writer(fid)\n",
    "        writer.writerows(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
