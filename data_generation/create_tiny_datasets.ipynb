{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"tiny\" datasets from larger sets\n",
    "This generates a smaller train/validation dataset of N per class (default to 100). Used to evaluate how well platforms perform with small input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import clarifai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params\n",
    "1. Insert datasets you want to make tiny\n",
    "2. Insert path to datasets, root level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the dataset of interest\n",
    "datasets = [\n",
    "    'fashion_mnist',    \n",
    "    'cifar10', \n",
    "    'uo_dress',\n",
    "    'mnist'\n",
    "]\n",
    "root_data = '{PATH_TO_ROOT_DATASETS}'\n",
    "n_per_class = 100\n",
    "suffix = 'tiny'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through datasets and make tiny train/val versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    dataset_path = os.path.join(root_data, d)\n",
    "    save_path = os.path.join(root_data, '{}_{}'.format(d, suffix))\n",
    "    save_train = os.path.join(save_path, 'train')\n",
    "    save_val = os.path.join(save_path, 'val')\n",
    "    save_test = os.path.join(save_path, 'test')\n",
    "    try:\n",
    "        os.makedirs(save_path)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(save_train)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(save_val)    \n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(save_test)    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Open CSV\n",
    "    labels_file = os.path.join(save_path, 'labels.csv')\n",
    "    fid = open(labels_file, \"w\")\n",
    "    writer = csv.writer(fid)\n",
    "\n",
    "    # Read in file locations and labels\n",
    "    labels = pd.read_csv(os.path.join(dataset_path, 'labels.csv'), header=None, dtype=str)\n",
    "    train_files = labels[labels[0].str.contains('(train|val)')].values\n",
    "    test_files = labels[labels[0].str.contains('test')].values\n",
    "\n",
    "    # Fill in training, select n_per_class_random\n",
    "    class_labels = np.sort(np.unique(train_files[:,1]))\n",
    "    new_labels = []\n",
    "    for cls in class_labels:\n",
    "        # get class subset\n",
    "        tfiles = train_files[train_files[:,1]==cls,:]\n",
    "        # Find N train\n",
    "        inds = np.random.choice(len(tfiles), size=n_per_class, replace=False)\n",
    "        keep_files = tfiles[inds,:]\n",
    "        keep_paths = keep_files[:,0]\n",
    "\n",
    "        # Copy files over\n",
    "        for p in keep_paths:\n",
    "            shutil.copyfile(os.path.join(dataset_path, p), os.path.join(save_path, p))\n",
    "\n",
    "        # Store off labels\n",
    "        for k in keep_files:\n",
    "            new_labels += [k.tolist()]\n",
    "\n",
    "        writer.writerows(keep_files)\n",
    "\n",
    "    # Copy all of test over\n",
    "    # Copy files over\n",
    "    for p in test_files[:,0]:\n",
    "        shutil.copyfile(os.path.join(dataset_path, p), os.path.join(save_path, p))\n",
    "\n",
    "    writer.writerows(test_files)\n",
    "\n",
    "    # Close file\n",
    "    fid.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
