{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_sets = []  # List ones to skip here\n",
    "\n",
    "root_data = '/data/shared/rework/gold_standard/'\n",
    "\n",
    "max_predict = 450\n",
    "\n",
    "# # rsanders@urbn key\n",
    "# api_key = 'GFIFLWTE6H36GHW2E2R2IYQIET7CKLCXSEENXDQEXDEPL4Q37R3BSAUP2ORC5B4KQDQAWLDXNJW6I45REOWLVXLCR25CABDXV37ZFAA'\n",
    "\n",
    "# t.szumowski key\n",
    "api_key = 'R4H75LZTZXUA77D5J3AVHEBHA765GUD7QQAAR52TA23MMW5A4DGABO7ZGURIXDX52ALU55GXBRYAZH5TAA6NVQEVM2PSZSVSGCSHFTY'\n",
    "\n",
    "url_pred = 'https://api.einstein.ai/v2/vision/predict'\n",
    "\n",
    "# List of dataset name and model IDs that were trained (from rsanders2@urbn.com account)\n",
    "# tiny's are from tom's t.szumowski@gmail.com account\n",
    "dataset_models = {\n",
    "#     'fashion_mnist_10p': 'SL4CGUDXTQ5ZT5JNXFDBWNFBRI',    \n",
    "#     'cifar10_20p': 'SRLOAZFACV2KSERFYIAI7GRSI4',    \n",
    "#     'uo_dress': 'MTC4BV3JMPYCKJ5NY4OYGALZBU',\n",
    "    'fashion_mnist_tiny': 'KRGDDZ7VNBD42ZXOOUOKMIU6MA',    \n",
    "    'cifar10_tiny': 'ZHCEZBCHL3IO467A7UZ6KISSUU',    \n",
    "    'uo_dress_tiny': 'DTSE3ZQVLLZKP5GMBNXFE7EXPY',    \n",
    "    'mnist_tiny': 'FYT46CRAU7R46BQF7CBXT64NIM',        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_path, model_id):\n",
    "    \n",
    "    d = os.path.basename(dataset_path)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    project_name = '{}_{}'.format(d, timestamp)    \n",
    "    \n",
    "    # parse data\n",
    "    labels = pd.read_csv(os.path.join(dataset_path, 'labels.csv'), header=None, dtype=str)\n",
    "    train_files = labels[labels[0].str.contains('(train|val)')].values\n",
    "    test_files = labels[labels[0].str.contains('test')].values    \n",
    "    \n",
    "    y_train = train_files[:,1]\n",
    "    class_labels = np.sort(np.unique(y_train))\n",
    "    \n",
    "    # Assign ground truth for test\n",
    "    y_true = test_files[:,1]\n",
    "\n",
    "    # Load in files for prediction \n",
    "    y_pred = []\n",
    "    scores = []\n",
    "    files = {}\n",
    "    test_filenames = test_files[:, 0]\n",
    "    test_filenames = [os.path.join(dataset_path, c) for c in test_filenames]\n",
    "    n_pred = len(test_filenames)\n",
    "    print('{} prediction files found.'.format(n_pred))\n",
    "    \n",
    "    # Clean up sizing\n",
    "    if n_pred > max_predict:\n",
    "        print('Number of prediction files ({}) exceeds maximum ({}). sampling by class down to max.'.format(n_pred, max_predict))\n",
    "        inds=np.random.choice(n_pred, max_predict, replace=False)\n",
    "        y_true = y_true[inds]\n",
    "        test_filenames = np.array(test_filenames)[inds].tolist() \n",
    "        print(\"New length: {}\".format(len(test_filenames)))\n",
    "    \n",
    "    # Worker for predictions\n",
    "    def predict_worker(filename):\n",
    "        basename = os.path.basename(filename)\n",
    "        cmd=\"curl -s -X POST \\\n",
    "          https://api.einstein.ai/v2/vision/predict \\\n",
    "          -H 'Authorization: Bearer {}' \\\n",
    "          -H 'Cache-Control: no-cache' \\\n",
    "          -H 'Content-Type: multipart/form-data' \\\n",
    "          -H 'content-type: multipart/form-data' \\\n",
    "          -F sampleContent=@{} \\\n",
    "          -F modelId={} \\\n",
    "          -F numResults={}\".format(api_key, filename, model_id, len(class_labels))\n",
    "            \n",
    "        try:\n",
    "            out = json.loads(os.popen(cmd).read())\n",
    "            pred_set = out['probabilities']\n",
    "\n",
    "            # Get sorted scores from prediction set\n",
    "            t_pred = None\n",
    "            t_scores = None            \n",
    "            t_names = np.array([z['label'] for z in pred_set])\n",
    "            t_scores = np.array([z['probability'] for z in pred_set])\n",
    "            sort_inds = np.argsort(t_names)\n",
    "            t_names = t_names[sort_inds]\n",
    "            t_scores = t_scores[sort_inds]\n",
    "            t_pred = t_names[np.argmax(t_scores)]        \n",
    "        except:\n",
    "            print('Error occured on prediction: {}. Skipping save.'.format(filename))\n",
    "\n",
    "        return basename, t_pred, t_scores\n",
    "\n",
    "    # Run parallel calls to make faster\n",
    "    t_start = time.time()\n",
    "    with Parallel(n_jobs=-1, verbose=5) as parallel:\n",
    "        results = parallel(delayed(predict_worker)(f) for f in test_filenames)\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print(len(results))\n",
    "    print('{:0.3f} secs elapsed for predicting {} images'.format(t_elapsed, len(test_filenames)))\n",
    "\n",
    "    # Parse the parallel output\n",
    "    returned_files = [r[0] for r in results]\n",
    "    y_pred = [r[1] for r in results]\n",
    "    scores = [r[2] for r in results]\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    print('Number of predictions: {}'.format(len(y_pred)))\n",
    "    print('Number of fails: {}.'.format(sum([yy is None for yy in y_pred])))\n",
    "\n",
    "    #\n",
    "    # Save results\n",
    "    #\n",
    "    save_file = '{}-results.p'.format(project_name)\n",
    "    save_dict = {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'scores': scores,\n",
    "        'class_labels': class_labels,\n",
    "        'model_name': project_name,\n",
    "        'model': None,\n",
    "        'train_files': train_files,\n",
    "        'test_files': test_files,\n",
    "        'returned_files': returned_files\n",
    "        }\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    print('Saved to {}'.format(save_file))\n",
    "\n",
    "    return save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING DATASET: /data/shared/rework/gold_standard/fashion_mnist_tiny\n",
      "10000 prediction files found.\n",
      "Number of prediction files (10000) exceeds maximum (450). sampling by class down to max.\n",
      "New length: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szumowskit1/venv/rework/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:   35.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "35.902 secs elapsed for predicting 450 images\n",
      "Number of predictions: 450\n",
      "Number of fails: 0.\n",
      "Saved to fashion_mnist_tiny_20180913221410-results.p\n",
      "Done!\n",
      "EXECUTING DATASET: /data/shared/rework/gold_standard/uo_dress_tiny\n",
      "545 prediction files found.\n",
      "Number of prediction files (545) exceeds maximum (450). sampling by class down to max.\n",
      "New length: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:   32.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "32.413 secs elapsed for predicting 450 images\n",
      "Number of predictions: 450\n",
      "Number of fails: 0.\n",
      "Saved to uo_dress_tiny_20180913221446-results.p\n",
      "Done!\n",
      "EXECUTING DATASET: /data/shared/rework/gold_standard/mnist_tiny\n",
      "10000 prediction files found.\n",
      "Number of prediction files (10000) exceeds maximum (450). sampling by class down to max.\n",
      "New length: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:   35.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "35.672 secs elapsed for predicting 450 images\n",
      "Number of predictions: 450\n",
      "Number of fails: 0.\n",
      "Saved to mnist_tiny_20180913221519-results.p\n",
      "Done!\n",
      "EXECUTING DATASET: /data/shared/rework/gold_standard/cifar10_tiny\n",
      "10000 prediction files found.\n",
      "Number of prediction files (10000) exceeds maximum (450). sampling by class down to max.\n",
      "New length: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   30.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "31.056 secs elapsed for predicting 450 images\n",
      "Number of predictions: 450\n",
      "Number of fails: 0.\n",
      "Saved to cifar10_tiny_20180913221555-results.p\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:   31.1s finished\n"
     ]
    }
   ],
   "source": [
    "for d,model_id in dataset_models.items():\n",
    "    if d in skip_sets:\n",
    "        continue\n",
    "        \n",
    "    # Get dataset key and directory\n",
    "    dataset_path = os.path.join(root_data, d)\n",
    "    print('EXECUTING DATASET: {}'.format(dataset_path))    \n",
    "    \n",
    "    # Run mega routine\n",
    "    evaluate(dataset_path, model_id)\n",
    "    \n",
    "    # Output\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
