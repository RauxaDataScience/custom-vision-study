{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarifai Dataset Evaluation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import clarifai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from clarifai.rest import ClarifaiApp\n",
    "from clarifai.rest import Image as ClImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params\n",
    "This script assumes you already manually uploaded the datasets into different groups, and you have an API key for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your API keys\n",
    "datasets = {\n",
    "    'fashion_mnist_10p': '{DATASET_SPECIFIC_API_KEY}',    \n",
    "    'cifar10_20p': '{DATASET_SPECIFIC_API_KEY}',    \n",
    "    'uo_dress': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'cifar10': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'fashion_mnist': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'mnist': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'fashion_mnist_tiny': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'cifar10_tiny': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'mnist_tiny': '{DATASET_SPECIFIC_API_KEY}',\n",
    "    'uo_dress_tiny': '{DATASET_SPECIFIC_API_KEY}'\n",
    "    }\n",
    "skip_sets = []  # List ones to skip here\n",
    "root_data = '{PATH_TO_ROOT_DATASETS}'\n",
    "n_batch = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield successive n-sized chunks from l. \n",
    "def divide_chunks(l, n):      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main train/test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(dataset_path, api_key):\n",
    "    \n",
    "    # Get dataset\n",
    "    d = os.path.basename(dataset_path)\n",
    "\n",
    "    #\n",
    "    # Get application using API Key\n",
    "    #\n",
    "    app = ClarifaiApp(api_key=api_key)    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Read in file locations and labels\n",
    "    #\n",
    "    labels = pd.read_csv(os.path.join(dataset_path, 'labels.csv'), header=None, dtype=str)\n",
    "    train_files = labels[labels[0].str.contains('(train|val)')].values\n",
    "    test_files = labels[labels[0].str.contains('test')].values\n",
    "\n",
    "    #\n",
    "    # Load in training images\n",
    "    # Create array of clarifai image objects (i.e. read in images to memory)\n",
    "    #\n",
    "    image_objs = []\n",
    "    print('Loading training images into memory...')\n",
    "    for row in train_files:\n",
    "        tfile = os.path.join(dataset_path, row[0])\n",
    "        tconcept = row[1]\n",
    "        tmeta = {'filename': tfile}\n",
    "        img_tmp = ClImage(filename=tfile, concepts=[tconcept], metadata=tmeta)\n",
    "        image_objs = image_objs + [img_tmp]\n",
    "    print('Number of training images: {}'.format(len(image_objs)))\n",
    "\n",
    "\n",
    "    #\n",
    "    # Create new model\n",
    "    #\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model_name = '{}_{}'.format(d, timestamp)\n",
    "    concepts = np.unique(train_files[:,1]).tolist()  # Get list of concept names\n",
    "    model = app.models.create(model_name, concepts=concepts)\n",
    "    print('Model Name: '.format(model_name))\n",
    "    print('Concepts: {}'.format(concepts))\n",
    "\n",
    "\n",
    "    #\n",
    "    # Upload training data in chunks\n",
    "    #\n",
    "    chunks = list(divide_chunks(image_objs, n_batch))\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print('Bulk upload chunk {} of {}...'.format(i+1, len(chunks)))\n",
    "        app.inputs.bulk_create_images(chunk)\n",
    "        \n",
    "\n",
    "    #\n",
    "    # Train model\n",
    "    # \n",
    "    print('Training model ...')\n",
    "    model = app.models.get(model_name)  # Retrieve in case it expired\n",
    "    model.train()\n",
    "    print('Sleeping for a few minutes to make sure model is done training ...')\n",
    "    time.sleep(240)\n",
    "\n",
    "    \n",
    "    #\n",
    "    # Load in prediction images\n",
    "    #\n",
    "    image_objs_test = []\n",
    "    print('Loading prediction images into memory...')\n",
    "    for row in test_files:\n",
    "        tfile = os.path.join(dataset_path, row[0])\n",
    "        #tconcept = row[1]\n",
    "        #tmeta = {'filename': tfile}\n",
    "        img_tmp = ClImage(filename=tfile)\n",
    "        image_objs_test = image_objs_test + [img_tmp]\n",
    "    print('Number of prediction images: {}'.format(len(image_objs_test)))\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Bulk predict on holdout test data in chunks\n",
    "    #\n",
    "    y_true = test_files[:,1]\n",
    "    class_labels = np.sort(np.unique(y_true))\n",
    "    model = app.models.get(model_name)  # Retrieve in case it expired\n",
    "\n",
    "\n",
    "    # Chunk it up!\n",
    "    y_pred = []\n",
    "    scores = []\n",
    "    chunks = list(divide_chunks(image_objs_test, n_batch))\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print('Predicting on chunk {} of {}...'.format(i+1, len(chunks)))\n",
    "        m = model.predict(chunk)\n",
    "\n",
    "        # Parse out predictions\n",
    "        for i,out in enumerate(m['outputs']):\n",
    "            pred_set = out['data']['concepts']\n",
    "\n",
    "            # Get sorted scores from prediction set\n",
    "            t_names = np.array([z['name'] for z in pred_set])\n",
    "            t_scores = np.array([z['value'] for z in pred_set])\n",
    "            sort_inds = np.argsort(t_names)\n",
    "            t_names = t_names[sort_inds]\n",
    "            t_scores = t_scores[sort_inds]\n",
    "            t_pred = t_names[np.argmax(t_scores)]\n",
    "\n",
    "            y_pred = y_pred + [t_pred]\n",
    "            scores = scores + [t_scores]\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    #\n",
    "    # Save results\n",
    "    #\n",
    "    save_file = '{}-results.p'.format(model_name)\n",
    "    save_dict = {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'scores': scores,\n",
    "        'class_labels': class_labels,\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'train_files': train_files,\n",
    "        'test_files': test_files\n",
    "        }\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    print('Saved to {}'.format(save_file))\n",
    "    \n",
    "    return save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Train/Test on Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,api_key in datasets.items():\n",
    "    if d in skip_sets:\n",
    "        continue\n",
    "        \n",
    "    # Get dataset key and directory\n",
    "    dataset_path = os.path.join(root_data, d)\n",
    "    print('EXECUTING DATASET: {}'.format(dataset_path))    \n",
    "    \n",
    "    # Run mega routine\n",
    "    train_and_test(dataset_path, api_key)\n",
    "    \n",
    "    # Output\n",
    "    print('Done! Be sure to press EVALUATE on the UI since that cannot be performed programmatically.\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
